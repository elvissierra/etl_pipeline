# ETL Pipeline Skeleton

This is a modular, reusable ETL (Extract, Transform, Load) pipeline skeleton using Python and SQLite, designed for quick adaptation in future data processing projects.

---

## ğŸ“ Project Structure
```
ETL_Pipeline/
â”œâ”€â”€ etl/                  # Core modules (extract, transform, load)
â”œâ”€â”€ config/               # Configuration and schema
â”œâ”€â”€ data/                 # Input/output data
â”œâ”€â”€ logs/                 # Logging outputs
â”œâ”€â”€ tests/                # Unit tests (optional)
â”œâ”€â”€ main.py               # Entry point
â”œâ”€â”€ .env                  # Environment variables
â””â”€â”€ requirements.txt      # Dependencies
```

---

## âš™ï¸ Configuration
All parameters can be configured in `config.py` and `.env`.

### Example `.env`
```
DATA_PATH=data/Test_Data.csv
DB_PATH=data/report_data.db
LOG_FILE=etl_errors.log
```

### Example `config.py`
```python
REQUIRED_COLUMNS = ["place_id", "edited_fields", ...]
FILTER_COLUMN = "all_customer_suggested_fields_edited"
FILTER_VALUE = "Yes"
TABLE_NAME = "report_data"
TABLE_SCHEMA = {
    "place_id": "INTEGER PRIMARY KEY",
    "edited_fields": "TEXT",
    ...
}
```

---

## ğŸš€ Running the Pipeline
```bash
python main.py --input path/to/input.csv --output path/to/output.db
```

If arguments are not provided, defaults from `.env` will be used.

---

## ğŸ§ª Testing
You can add tests using `pytest` in the `tests/` directory to validate transformations and schema logic.

---

## âœ… Features
- Modular `extract`, `transform`, `load` functions
- Configurable schema and column validation
- CLI + `.env` flexibility
- Basic logging with optional log rotation

---

## ğŸ“¦ Dependencies
```bash
pip install -r requirements.txt
```
Required:
- `pandas`
- `python-dotenv`
```
# requirements.txt
pandas
python-dotenv
```

---

## ğŸ“Œ License
MIT or your chosen license.

