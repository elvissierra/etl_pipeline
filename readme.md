
# ETL Pipeline Skeleton

This is a modular, reusable ETL (Extract, Transform, Load) pipeline skeleton using Python and SQLite, designed for quick adaptation in future data processing projects.

---

## ğŸ“ Project Structure
```
ETL_Pipeline/ # current directory
â”œâ”€â”€ auto_report_pipeline/
    â”œâ”€â”€ __init__
    â”œâ”€â”€ AR_ETL
    â”œâ”€â”€ extract
    â”œâ”€â”€ report_generator
    â”œâ”€â”€ transform
    â”œâ”€â”€ utils
    â””â”€â”€ 
```

---

## âš™ï¸ Configuration
All parameters can be configured in `config.py` and `.env`.

### Example `.env`
```
DATA_PATH=data/Test_Data.csv
DB_PATH=data/report_data.db
LOG_FILE=etl_errors.log
```

### Example `config.py`
```python
REQUIRED_COLUMNS = ["place_id", "edited_fields", ...]
FILTER_COLUMN = "fields_edited"
FILTER_VALUE = "Yes"
TABLE_NAME = "report_data"
TABLE_SCHEMA = {
    "place_id": "INTEGER PRIMARY KEY",
    "edited_fields": "TEXT",
    ...
}
```

---

## ğŸš€ Running the Pipeline
```bash
python auto_report.py --config-path csv_files/report_config.csv   

```

If arguments are not provided, defaults from `.env` will be used.

---

## ğŸ§ª Testing
You can add tests using `pytest` in the `tests/` directory to validate transformations and schema logic.

---

## âœ… Features
- Modular `extract`, `transform`, `load` functions
- Configurable schema and column validation
- CLI + `.env` flexibility
- Basic logging with optional log rotation

---

## ğŸ“¦ Dependencies
```bash
pip install -r requirements.txt
```
Required:
- `pandas`
- `python-dotenv`
```
# requirements.txt
pandas
python-dotenv
```

---
